{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Model\n",
    "\n",
    "In this notebook, we gonna present how to make a Language Model using LSTMs. This is the \"old-school\" way to make language models. Recently, with the introduction of the Transformer architecture, one can successfully make a Language Model with better overall quality instead of using LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from practicalnlp import settings\n",
    "from practicalnlp.models import *\n",
    "from practicalnlp.training import *\n",
    "from practicalnlp.data import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "Here we load all the data with `batch_size = 20`. It's important to note that we subdivide data with 2 parameters: `nctx` and `batch_size`. `nctx` is the number of words we are using in a single pass of a training phase. For example, the figure below ilustrates each *step* in the training phase for `nctx = 3` over a single `batch_size` of the entire sentence below.\n",
    "\n",
    "\n",
    "<img src=\"training_step_lm.svg\" width=\"800\" />\n",
    "<!--- [svg](training_step_lm.svg)> --->\n",
    "\n",
    "Arrows indicate that the origin word is trying to predict the next word in the `nctx` window. When the last word of the `nctx` window is processed, the window is translated by `nctx` words and the process repeats until it reads the entire batch. The `nctx` param is also known as `bptt` (*backpropagation through time*), and is the name used in the official PyTorch tutorial for Language Modeling.\n",
    "\n",
    "Although this example shows the execution for only a single batch, in practice, we do it for all batchs at the same time. It might be easy to understand how it can be done in practice with a 2-dimensional tensor (one dimension for batch size, and other for the sequence length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "nctx = 35\n",
    "TRAIN = settings.WIKI_TRAIN_DATA\n",
    "VALID = settings.WIKI_VALID_DATA\n",
    "reader = WordDatasetReader(nctx)\n",
    "reader.build_vocab((TRAIN,))\n",
    "\n",
    "train_set = reader.load(TRAIN, batch_size)\n",
    "valid_set = reader.load(VALID, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 21274623 parameters\n",
      "EPOCH 1\n",
      "=================================\n",
      "Training Results\n",
      "average_train_loss 7.032219 (7.693288)\n",
      "average_train_loss 6.941036 (7.207089)\n",
      "average_train_loss 6.441201 (6.979663)\n",
      "average_train_loss 6.652155 (6.837320)\n",
      "average_train_loss 6.503147 (6.733491)\n",
      "average_train_loss 6.221502 (6.655999)\n",
      "average_train_loss 6.215572 (6.598819)\n",
      "average_train_loss 6.135547 (6.548828)\n",
      "average_train_loss 6.034210 (6.503680)\n",
      "average_train_loss 5.849336 (6.467311)\n",
      "average_train_loss 6.151655 (6.435877)\n",
      "average_train_loss 5.897994 (6.409550)\n",
      "average_train_loss 6.146118 (6.387423)\n",
      "average_train_loss 6.198409 (6.361967)\n",
      "average_train_loss 6.185694 (6.344496)\n",
      "average_train_loss 6.094780 (6.329319)\n",
      "average_train_loss 6.078131 (6.308456)\n",
      "average_train_loss 5.808933 (6.288046)\n",
      "average_train_loss 5.748820 (6.272158)\n",
      "average_train_loss 5.981767 (6.255495)\n",
      "average_train_loss 5.676554 (6.238284)\n",
      "average_train_loss 5.675500 (6.219382)\n",
      "average_train_loss 5.981535 (6.202112)\n",
      "average_train_loss 5.863875 (6.189984)\n",
      "average_train_loss 5.913692 (6.176306)\n",
      "average_train_loss 5.784894 (6.165209)\n",
      "average_train_loss 5.440230 (6.150763)\n",
      "average_train_loss 5.899689 (6.136422)\n",
      "average_train_loss 5.503354 (6.123079)\n",
      "{'train_elapsed_min': 1.423075254758199, 'average_train_loss': 6.109354144009096, 'train_ppl': 450.0479551151617}\n",
      "Validation Results\n",
      "{'valid_elapsed_min': 0.03654532829920451, 'average_valid_loss': 5.546387901613789, 'average_valid_word_ppl': 256.3100646484859}\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(len(reader.vocab), 512, 512)\n",
    "model.to('cuda:0')\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {num_params} parameters\") \n",
    "\n",
    "\n",
    "learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(learnable_params, lr=0.001)\n",
    "fit_lm(model, optimizer, 1, batch_size, nctx, train_set, valid_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
