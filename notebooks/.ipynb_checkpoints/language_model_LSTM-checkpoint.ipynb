{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Model\n",
    "\n",
    "In this notebook, we are going to make a Language Model using LSTMs. This is the \"old-school\" way to make language models. Recently, with the introduction of the Transformer architecture, one can successfully make a Language Model with better overall quality instead of using LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from practicalnlp import settings\n",
    "from practicalnlp.models import *\n",
    "from practicalnlp.training import *\n",
    "from practicalnlp.data import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "Here we load all data with `batch_size = 20`. It's important to note that we subdivide data with 2 parameters: `nctx` and `batch_size`. `nctx` is the number of words we are using in a single pass of a training phase. For example, the figure below ilustrates each *step* in the training phase for `nctx = 3` over a single `batch_size` of the entire sentence below.\n",
    "\n",
    "\n",
    "<img src=\"training_step_lm.svg\" width=\"800\" />\n",
    "<!--- [svg](training_step_lm.svg)> --->\n",
    "\n",
    "Arrows indicate that the origin word is trying to predict the next word in the `nctx` window. When the last word of the `nctx` window is processed, the window is translated by `nctx` words and the process repeats until it reads the entire batch. The `nctx` param is also known as `bptt` (*backpropagation through time*), and is the name used in the official PyTorch tutorial for Language Modeling.\n",
    "\n",
    "Although this example shows the execution for only a single batch, in practice, we do it for all batchs at the same time. It might be easy to understand how it can be done in practice with a 2-dimensional tensor (one dimension for batch size, and other for the sequence length). In the code below, we do it using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "nctx = 35\n",
    "TRAIN = settings.WIKI_TRAIN_DATA\n",
    "VALID = settings.WIKI_VALID_DATA\n",
    "reader = WordDatasetReader(nctx)\n",
    "reader.build_vocab((TRAIN,))\n",
    "\n",
    "train_set = reader.load(TRAIN, batch_size)\n",
    "valid_set = reader.load(VALID, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 104431])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 21274623 parameters\n",
      "EPOCH 1\n",
      "=================================\n",
      "Training Results\n",
      "average_train_loss 7.039359 (7.674278)\n",
      "average_train_loss 6.940264 (7.212206)\n",
      "average_train_loss 6.476130 (6.988302)\n",
      "average_train_loss 6.665786 (6.845469)\n",
      "average_train_loss 6.520135 (6.741749)\n",
      "average_train_loss 6.277775 (6.662471)\n",
      "average_train_loss 6.214710 (6.606091)\n",
      "average_train_loss 6.106718 (6.555020)\n",
      "average_train_loss 6.054318 (6.509634)\n",
      "average_train_loss 5.910986 (6.472560)\n",
      "average_train_loss 6.157823 (6.441040)\n",
      "average_train_loss 5.861746 (6.414230)\n",
      "average_train_loss 6.177941 (6.391719)\n",
      "average_train_loss 6.130061 (6.365675)\n",
      "average_train_loss 6.221581 (6.348069)\n",
      "average_train_loss 6.110457 (6.332552)\n",
      "average_train_loss 6.081135 (6.311838)\n",
      "average_train_loss 5.832395 (6.291147)\n",
      "average_train_loss 5.744493 (6.275616)\n",
      "average_train_loss 6.009275 (6.259222)\n",
      "average_train_loss 5.699287 (6.242274)\n",
      "average_train_loss 5.692174 (6.223206)\n",
      "average_train_loss 5.925848 (6.205693)\n",
      "average_train_loss 5.840515 (6.193715)\n",
      "average_train_loss 5.912480 (6.179914)\n",
      "average_train_loss 5.742425 (6.168865)\n",
      "average_train_loss 5.496578 (6.154583)\n",
      "average_train_loss 5.848550 (6.140081)\n",
      "average_train_loss 5.518538 (6.126692)\n",
      "{'train_elapsed_min': 1.3755362947781882, 'average_train_loss': 6.112885558920901, 'train_ppl': 451.64007073070445}\n",
      "Validation Results\n",
      "{'valid_elapsed_min': 0.03729297717412313, 'average_valid_loss': 5.544584132779029, 'average_valid_word_ppl': 255.8481572541566}\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(len(reader.vocab), 512, 512)\n",
    "model.to('cuda:0')\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {num_params} parameters\") \n",
    "\n",
    "\n",
    "learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(learnable_params, lr=0.001)\n",
    "fit_lm(model, optimizer, 1, batch_size, nctx, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModel(len(reader.vocab), 512, 2, 200, 2, 0.2)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {num_params} parameters\") \n",
    "\n",
    "\n",
    "learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(learnable_params, lr=0.001)\n",
    "fit_lm(model, optimizer, 1, batch_size, nctx, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the American album were 28 â€“ 3 . During 1907 , suspicions remained the first time since 1972 with him just ...\n"
     ]
    }
   ],
   "source": [
    "def sample(model, index2word, start_word='the', maxlen=20):\n",
    "  \n",
    "\n",
    "    model.eval() \n",
    "    words = [start_word]\n",
    "    x = torch.tensor(reader.vocab.get(start_word)).long().reshape(1, 1).to('cuda:0')\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(20):\n",
    "            output, hidden = model(x, hidden)\n",
    "            word_softmax = output.squeeze().exp().cpu()\n",
    "            selected = torch.multinomial(word_softmax, 1)[0]\n",
    "            x.fill_(selected)\n",
    "            word = index2word[selected.item()]\n",
    "            words.append(word)\n",
    "    words.append('...')\n",
    "    return words\n",
    "\n",
    "index2word = {i: w for w, i in reader.vocab.items()}\n",
    "words = sample(model, index2word)\n",
    "print(' '.join(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
