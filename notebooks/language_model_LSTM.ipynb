{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Model\n",
    "\n",
    "In this notebook, we are going to make a Language Model using LSTMs. This is the \"old-school\" way to make language models. Recently, with the introduction of the Transformer architecture, one can successfully make a Language Model with better overall quality instead of using LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from practicalnlp import settings\n",
    "from practicalnlp.models import *\n",
    "from practicalnlp.training import *\n",
    "from practicalnlp.data import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "Here we load all data with `batch_size = 20`. It's important to note that we subdivide data with 2 parameters: `nctx` and `batch_size`. `nctx` is the number of words we are using in a single pass of a training phase. For example, the figure below ilustrates each *step* in the training phase for `nctx = 3` over a single `batch_size` of the entire sentence below.\n",
    "\n",
    "\n",
    "<img src=\"training_step_lm.svg\" width=\"800\" />\n",
    "<!--- [svg](training_step_lm.svg)> --->\n",
    "\n",
    "Arrows indicate that the origin word is trying to predict the next word in the `nctx` window. When the last word of the `nctx` window is processed, the window is translated by `nctx` words and the process repeats until it reads the entire batch. The `nctx` param is also known as `bptt` (*backpropagation through time*), and is the name used in the official PyTorch tutorial for Language Modeling.\n",
    "\n",
    "Although this example shows the execution for only a single batch, in practice, we do it for all batchs at the same time. It might be easy to understand how it can be done in practice with a 2-dimensional tensor (one dimension for batch size, and other for the sequence length). In the code below, we do it using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "nctx = 35\n",
    "TRAIN = settings.WIKI_TRAIN_DATA\n",
    "VALID = settings.WIKI_VALID_DATA\n",
    "reader = WordDatasetReader(nctx)\n",
    "reader.build_vocab((TRAIN,))\n",
    "\n",
    "train_set = reader.load(TRAIN, batch_size)\n",
    "valid_set = reader.load(VALID, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 104431])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 21274623 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "=================================\n",
      "Training Results\n",
      "average_train_loss 6.993510 (7.717011)\n",
      "average_train_loss 6.968325 (7.219686)\n",
      "average_train_loss 6.436844 (6.992108)\n",
      "average_train_loss 6.624825 (6.847209)\n",
      "average_train_loss 6.506425 (6.741867)\n",
      "average_train_loss 6.232456 (6.661590)\n",
      "average_train_loss 6.241143 (6.604560)\n",
      "average_train_loss 6.209402 (6.553402)\n",
      "average_train_loss 6.053204 (6.508152)\n",
      "average_train_loss 5.882931 (6.471613)\n",
      "average_train_loss 6.114008 (6.439420)\n",
      "average_train_loss 5.837910 (6.412545)\n",
      "average_train_loss 6.178204 (6.389939)\n",
      "average_train_loss 6.141484 (6.364485)\n",
      "average_train_loss 6.193256 (6.346820)\n",
      "average_train_loss 6.029351 (6.331054)\n",
      "average_train_loss 6.109805 (6.309967)\n",
      "average_train_loss 5.808486 (6.289859)\n",
      "average_train_loss 5.751597 (6.273885)\n",
      "average_train_loss 5.986725 (6.256851)\n",
      "average_train_loss 5.706111 (6.239590)\n",
      "average_train_loss 5.704154 (6.220216)\n",
      "average_train_loss 5.918287 (6.202540)\n",
      "average_train_loss 5.850075 (6.190269)\n",
      "average_train_loss 5.963675 (6.176846)\n",
      "average_train_loss 5.765429 (6.166153)\n",
      "average_train_loss 5.467043 (6.151880)\n",
      "average_train_loss 5.867497 (6.137282)\n",
      "average_train_loss 5.505663 (6.123832)\n",
      "{'train_elapsed_min': 1.3444259683291118, 'average_train_loss': 6.110074019296194, 'train_ppl': 450.3720501560674}\n",
      "Validation Results\n",
      "{'valid_elapsed_min': 0.03240746259689331, 'average_valid_loss': 5.546152776287448, 'average_valid_word_ppl': 256.24980674524807}\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(len(reader.vocab), 512, 512)\n",
    "model.to('cuda:0')\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {num_params} parameters\") \n",
    "\n",
    "\n",
    "learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(learnable_params, lr=0.001)\n",
    "fit_lm(model, optimizer, 1, batch_size, nctx, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the American album were 28 â€“ 3 . During 1907 , suspicions remained the first time since 1972 with him just ...\n"
     ]
    }
   ],
   "source": [
    "def sample(model, index2word, start_word='the', maxlen=20):\n",
    "  \n",
    "\n",
    "    model.eval() \n",
    "    words = [start_word]\n",
    "    x = torch.tensor(reader.vocab.get(start_word)).long().reshape(1, 1).to('cuda:0')\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(20):\n",
    "            output, hidden = model(x, hidden)\n",
    "            word_softmax = output.squeeze().exp().cpu()\n",
    "            selected = torch.multinomial(word_softmax, 1)[0]\n",
    "            x.fill_(selected)\n",
    "            word = index2word[selected.item()]\n",
    "            words.append(word)\n",
    "    words.append('...')\n",
    "    return words\n",
    "\n",
    "index2word = {i: w for w, i in reader.vocab.items()}\n",
    "words = sample(model, index2word)\n",
    "print(' '.join(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
