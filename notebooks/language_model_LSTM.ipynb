{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Language Model\n",
    "\n",
    "In this notebook, we are going to make a Language Model using LSTMs. This is the \"old-school\" way to make language models. Recently, with the introduction of the Transformer architecture, one can successfully make a Language Model with better overall quality instead of using LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from practicalnlp import settings\n",
    "from practicalnlp.models import *\n",
    "from practicalnlp.training import *\n",
    "from practicalnlp.data import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "Here we load all data with `batch_size = 20`. It's important to note that we subdivide data with 2 parameters: `nctx` and `batch_size`. `nctx` is the number of words we are using in a single pass of a training phase. For example, the figure below ilustrates each *step* in the training phase for `nctx = 3` over a single `batch_size` of the entire sentence below.\n",
    "\n",
    "\n",
    "<img src=\"training_step_lm.svg\" width=\"800\" />\n",
    "<!--- [svg](training_step_lm.svg)> --->\n",
    "\n",
    "Arrows indicate that the origin word is trying to predict the next word in the `nctx` window. When the last word of the `nctx` window is processed, the window is translated by `nctx` words and the process repeats until it reads the entire batch. The `nctx` param is also known as `bptt` (*backpropagation through time*), and is the name used in the official PyTorch tutorial for Language Modeling.\n",
    "\n",
    "Although this example shows the execution for only a single batch, in practice, we do it for all batchs at the same time. It might be easy to understand how it can be done in practice with a 2-dimensional tensor (one dimension for batch size, and other for the sequence length). In the code below, we do it using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "nctx = 35\n",
    "TRAIN = settings.WIKI_TRAIN_DATA\n",
    "VALID = settings.WIKI_VALID_DATA\n",
    "reader = WordDatasetReader(nctx)\n",
    "reader.build_vocab((TRAIN,))\n",
    "\n",
    "train_set = reader.load(TRAIN, batch_size)\n",
    "valid_set = reader.load(VALID, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 104431])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 21274623 parameters\n",
      "EPOCH 1\n",
      "=================================\n",
      "Training Results\n",
      "average_train_loss 7.093464 (7.688572)\n",
      "average_train_loss 6.886086 (7.213753)\n",
      "average_train_loss 6.399582 (6.992481)\n",
      "average_train_loss 6.709525 (6.850131)\n",
      "average_train_loss 6.531233 (6.745780)\n",
      "average_train_loss 6.296781 (6.665744)\n",
      "average_train_loss 6.202570 (6.608568)\n",
      "average_train_loss 6.164864 (6.558448)\n",
      "average_train_loss 6.103103 (6.514183)\n",
      "average_train_loss 5.831127 (6.476745)\n",
      "average_train_loss 6.082990 (6.445060)\n",
      "average_train_loss 5.877787 (6.417976)\n",
      "average_train_loss 6.173513 (6.395627)\n",
      "average_train_loss 6.158544 (6.369488)\n",
      "average_train_loss 6.182152 (6.351835)\n",
      "average_train_loss 6.053542 (6.336367)\n",
      "average_train_loss 6.132944 (6.315442)\n",
      "average_train_loss 5.806489 (6.295173)\n",
      "average_train_loss 5.750424 (6.279107)\n",
      "average_train_loss 5.949520 (6.262082)\n",
      "average_train_loss 5.634626 (6.244754)\n",
      "average_train_loss 5.706845 (6.225194)\n",
      "average_train_loss 5.873329 (6.207461)\n",
      "average_train_loss 5.798521 (6.195122)\n",
      "average_train_loss 5.973289 (6.181318)\n",
      "average_train_loss 5.805297 (6.170506)\n",
      "average_train_loss 5.569892 (6.156039)\n",
      "average_train_loss 5.896243 (6.141452)\n",
      "average_train_loss 5.482777 (6.127845)\n",
      "{'train_elapsed_min': 1.349797825018565, 'average_train_loss': 6.114175667988147, 'train_ppl': 452.2231116934351}\n",
      "Validation Results\n",
      "{'valid_elapsed_min': 0.036688764890034996, 'average_valid_loss': 5.554669804726878, 'average_valid_word_ppl': 258.4416142308211}\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(len(reader.vocab), 512, 512)\n",
    "model.to('cuda:0')\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {num_params} parameters\") \n",
    "\n",
    "\n",
    "learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(learnable_params, lr=0.001)\n",
    "fit_lm(model, optimizer, 1, batch_size, nctx, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the game . Right also record a obey Archive in nine @-@ books , with around 1 yards pounder . Larry ...\n"
     ]
    }
   ],
   "source": [
    "def sample(model, index2word, start_word='the', maxlen=20):\n",
    "  \n",
    "\n",
    "    model.eval() \n",
    "    words = [start_word]\n",
    "    x = torch.tensor(reader.vocab.get(start_word)).long().reshape(1, 1).to('cuda:0')\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(20):\n",
    "            output, hidden = model(x, hidden)\n",
    "            word_softmax = output.squeeze().exp().cpu()\n",
    "            selected = torch.multinomial(word_softmax, 1)[0]\n",
    "            x.fill_(selected)\n",
    "            word = index2word[selected.item()]\n",
    "            words.append(word)\n",
    "    words.append('...')\n",
    "    return words\n",
    "\n",
    "index2word = {i: w for w, i in reader.vocab.items()}\n",
    "words = sample(model, index2word)\n",
    "print(' '.join(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
